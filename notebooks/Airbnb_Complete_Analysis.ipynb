{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77eb4ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook location: /Users/macbookpro/Documents/more_projects/Airbnb_Analysis_Project\n",
      "Project root: /Users/macbookpro/Documents/more_projects/Airbnb_Analysis_Project\n",
      "Working directory changed to: /Users/macbookpro/Documents/more_projects/Airbnb_Analysis_Project\n",
      "\n",
      "✓ Created/Verified folder: data/raw\n",
      "✓ Created/Verified folder: data/processed\n",
      "✓ Created/Verified folder: visualizations/eda\n",
      "✓ Created/Verified folder: visualizations/modeling\n",
      "✓ Created/Verified folder: visualizations/clustering\n",
      "✓ Created/Verified folder: models\n",
      "✓ Created/Verified folder: reports\n",
      "✓ Created/Verified folder: config\n",
      "✓ Created/Verified folder: src\n",
      "\n",
      "================================================================================\n",
      "PROJECT SETUP COMPLETE\n",
      "================================================================================\n",
      "\n",
      "All folders created in: /Users/macbookpro/Documents/more_projects/Airbnb_Analysis_Project\n",
      "\n",
      "Folder structure verified:\n",
      "\n",
      "Airbnb_Analysis_Project/\n",
      "├── data/\n",
      "│   ├── raw/\n",
      "│   └── processed/\n",
      "├── visualizations/\n",
      "│   ├── eda/\n",
      "│   ├── modeling/\n",
      "│   └── clustering/\n",
      "├── models/\n",
      "├── reports/\n",
      "├── config/\n",
      "└── src/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 0: CREATE FOLDER STRUCTURE ====================\n",
    "\"\"\"\n",
    "SETUP: Create all necessary folders for the project\n",
    "JUSTIFICATION: Ensures all output directories exist before saving files\n",
    "NOTE: Creates folders in PROJECT ROOT, not in notebooks folder\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get project root directory (go up one level from notebooks folder)\n",
    "notebook_dir = os.getcwd()\n",
    "print(f\"Current notebook location: {notebook_dir}\")\n",
    "\n",
    "if notebook_dir.endswith('notebooks'):\n",
    "    project_root = os.path.dirname(notebook_dir)\n",
    "else:\n",
    "    project_root = notebook_dir\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory changed to: {os.getcwd()}\\n\")\n",
    "\n",
    "# Create folder structure (now relative to project root)\n",
    "folders = [\n",
    "    'data/raw',\n",
    "    'data/processed',\n",
    "    'visualizations/eda',\n",
    "    'visualizations/modeling',\n",
    "    'visualizations/clustering',\n",
    "    'models',\n",
    "    'reports',\n",
    "    'config',\n",
    "    'src'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    print(f\"✓ Created/Verified folder: {folder}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROJECT SETUP COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll folders created in: {project_root}\")\n",
    "print(\"\\nFolder structure verified:\")\n",
    "print(\"\"\"\n",
    "Airbnb_Analysis_Project/\n",
    "├── data/\n",
    "│   ├── raw/\n",
    "│   └── processed/\n",
    "├── visualizations/\n",
    "│   ├── eda/\n",
    "│   ├── modeling/\n",
    "│   └── clustering/\n",
    "├── models/\n",
    "├── reports/\n",
    "├── config/\n",
    "└── src/\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd25eb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset loaded successfully!\n",
      "Shape: 48,895 rows × 16 columns\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/raw/MinoAI_dataset.csv')\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a105c11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values Summary:\n",
      "           Column  Missing_Count  Missing_Percentage\n",
      "reviews_per_month          10052           20.558339\n",
      "      last_review          10052           20.558339\n",
      "        host_name             21            0.042949\n",
      "             name             16            0.032723\n",
      "\n",
      "✓ Saved: visualizations/eda/01_missing_values.png\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 2: MISSING VALUES ANALYSIS ====================\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing_Count': missing.values,\n",
    "    'Missing_Percentage': missing_pct.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "# SAVE: Missing values visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "missing_df_sorted = missing_df.sort_values('Missing_Percentage', ascending=True)\n",
    "ax.barh(missing_df_sorted['Column'], missing_df_sorted['Missing_Percentage'], color='coral', edgecolor='black')\n",
    "ax.set_xlabel('Percentage of Missing Data (%)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Missing Values Analysis', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/eda/01_missing_values.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Saved: visualizations/eda/01_missing_values.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12ea689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics Summary:\n",
      "                id       host_id  latitude  longitude     price  \\\n",
      "count     48895.00  4.889500e+04  48895.00   48895.00  48895.00   \n",
      "mean   19017143.24  6.762001e+07     40.73     -73.95    152.72   \n",
      "std    10983108.39  7.861097e+07      0.05       0.05    240.15   \n",
      "min        2539.00  2.438000e+03     40.50     -74.24      0.00   \n",
      "25%     9471945.00  7.822033e+06     40.69     -73.98     69.00   \n",
      "50%    19677284.00  3.079382e+07     40.72     -73.96    106.00   \n",
      "75%    29152178.50  1.074344e+08     40.76     -73.94    175.00   \n",
      "max    36487245.00  2.743213e+08     40.91     -73.71  10000.00   \n",
      "\n",
      "       minimum_nights  number_of_reviews  reviews_per_month  \\\n",
      "count        48895.00           48895.00           38843.00   \n",
      "mean             7.03              23.27               1.37   \n",
      "std             20.51              44.55               1.68   \n",
      "min              1.00               0.00               0.01   \n",
      "25%              1.00               1.00               0.19   \n",
      "50%              3.00               5.00               0.72   \n",
      "75%              5.00              24.00               2.02   \n",
      "max           1250.00             629.00              58.50   \n",
      "\n",
      "       calculated_host_listings_count  availability_365  \n",
      "count                        48895.00          48895.00  \n",
      "mean                             7.14            112.78  \n",
      "std                             32.95            131.62  \n",
      "min                              1.00              0.00  \n",
      "25%                              1.00              0.00  \n",
      "50%                              1.00             45.00  \n",
      "75%                              2.00            227.00  \n",
      "max                            327.00            365.00  \n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 3: DESCRIPTIVE STATISTICS ====================\n",
    "print(\"\\nDescriptive Statistics Summary:\")\n",
    "print(df.describe().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490eb0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Cleaning:\n",
      "  Removed 11 rows with invalid prices\n",
      "  Removed 0 rows with negative minimum nights\n",
      "  Cleaned dataset: 48,884 rows\n",
      "\n",
      "✓ Saved: data/processed/data_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 4: DATA CLEANING ====================\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Remove invalid prices\n",
    "initial_rows = len(df_clean)\n",
    "df_clean = df_clean[df_clean['price'] > 0]\n",
    "print(f\"\\nData Cleaning:\")\n",
    "print(f\"  Removed {initial_rows - len(df_clean)} rows with invalid prices\")\n",
    "\n",
    "# Remove negative minimum nights\n",
    "initial_rows = len(df_clean)\n",
    "df_clean = df_clean[df_clean['minimum_nights'] >= 0]\n",
    "print(f\"  Removed {initial_rows - len(df_clean)} rows with negative minimum nights\")\n",
    "\n",
    "# Fill missing reviews_per_month\n",
    "median_rpm = df_clean['reviews_per_month'].median()\n",
    "df_clean['reviews_per_month'].fillna(median_rpm, inplace=True)\n",
    "\n",
    "# Fill missing last_review\n",
    "df_clean['last_review'].fillna('No Review', inplace=True)\n",
    "\n",
    "print(f\"  Cleaned dataset: {df_clean.shape[0]:,} rows\")\n",
    "\n",
    "# Save cleaned data\n",
    "df_clean.to_csv('data/processed/data_cleaned.csv', index=False)\n",
    "print(f\"\\n✓ Saved: data/processed/data_cleaned.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a33143d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Univariate Analysis Visualizations...\n",
      "✓ Saved: visualizations/eda/02_univariate_analysis.png\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 5: UNIVARIATE ANALYSIS ====================\n",
    "print(\"\\nGenerating Univariate Analysis Visualizations...\")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Price distributions\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "ax1.hist(df_clean['price'], bins=100, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax1.set_title('Price Distribution (All Data)', fontweight='bold')\n",
    "ax1.set_xlabel('Price ($)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.axvline(df_clean['price'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${df_clean[\"price\"].mean():.0f}')\n",
    "ax1.axvline(df_clean['price'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: ${df_clean[\"price\"].median():.0f}')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "price_filtered = df_clean[df_clean['price'] <= df_clean['price'].quantile(0.95)]\n",
    "ax2.hist(price_filtered['price'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "ax2.set_title('Price Distribution (Excluding Top 5%)', fontweight='bold')\n",
    "ax2.set_xlabel('Price ($)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "ax3.boxplot(df_clean['price'], vert=True)\n",
    "ax3.set_title('Price Box Plot', fontweight='bold')\n",
    "ax3.set_ylabel('Price ($)')\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# Minimum nights\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "min_nights_filtered = df_clean[df_clean['minimum_nights'] <= df_clean['minimum_nights'].quantile(0.95)]\n",
    "ax4.hist(min_nights_filtered['minimum_nights'], bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "ax4.set_title('Minimum Nights (Excluding Top 5%)', fontweight='bold')\n",
    "ax4.set_xlabel('Days')\n",
    "ax4.set_ylabel('Frequency')\n",
    "\n",
    "# Number of reviews\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "ax5.hist(df_clean['number_of_reviews'], bins=50, color='purple', edgecolor='black', alpha=0.7)\n",
    "ax5.set_title('Number of Reviews Distribution', fontweight='bold')\n",
    "ax5.set_xlabel('Number of Reviews')\n",
    "ax5.set_ylabel('Frequency')\n",
    "\n",
    "# Reviews per month\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "rpm_data = df_clean['reviews_per_month'].dropna()\n",
    "ax6.hist(rpm_data, bins=50, color='teal', edgecolor='black', alpha=0.7)\n",
    "ax6.set_title('Reviews Per Month Distribution', fontweight='bold')\n",
    "ax6.set_xlabel('Reviews/Month')\n",
    "ax6.set_ylabel('Frequency')\n",
    "\n",
    "# Availability\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "ax7.hist(df_clean['availability_365'], bins=50, color='darkgreen', edgecolor='black', alpha=0.7)\n",
    "ax7.set_title('Availability (365 Days)', fontweight='bold')\n",
    "ax7.set_xlabel('Days Available')\n",
    "ax7.set_ylabel('Frequency')\n",
    "\n",
    "# Host listings count\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "host_count_filtered = df_clean[df_clean['calculated_host_listings_count'] <= df_clean['calculated_host_listings_count'].quantile(0.95)]\n",
    "ax8.hist(host_count_filtered['calculated_host_listings_count'], bins=50, color='darkred', edgecolor='black', alpha=0.7)\n",
    "ax8.set_title('Host Listings Count (Excluding Top 5%)', fontweight='bold')\n",
    "ax8.set_xlabel('Number of Listings')\n",
    "ax8.set_ylabel('Frequency')\n",
    "\n",
    "# Room type\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "room_counts = df_clean['room_type'].value_counts()\n",
    "ax9.bar(range(len(room_counts)), room_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'], edgecolor='black')\n",
    "ax9.set_xticks(range(len(room_counts)))\n",
    "ax9.set_xticklabels(room_counts.index, rotation=45, ha='right')\n",
    "ax9.set_title('Room Type Distribution', fontweight='bold')\n",
    "ax9.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/eda/02_univariate_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: visualizations/eda/02_univariate_analysis.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87367b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Bivariate Analysis Visualizations...\n",
      "✓ Saved: visualizations/eda/03_bivariate_analysis.png\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 6: BIVARIATE ANALYSIS ====================\n",
    "print(\"\\nGenerating Bivariate Analysis Visualizations...\")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Price by room type\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "df_clean.boxplot(column='price', by='room_type', ax=ax1)\n",
    "ax1.set_title('Price Distribution by Room Type', fontweight='bold')\n",
    "ax1.set_xlabel('Room Type')\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.get_figure().suptitle('')\n",
    "\n",
    "# Price by neighbourhood\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "df_clean.boxplot(column='price', by='neighbourhood_group', ax=ax2)\n",
    "ax2.set_title('Price Distribution by Neighbourhood Group', fontweight='bold')\n",
    "ax2.set_xlabel('Neighbourhood Group')\n",
    "ax2.set_ylabel('Price ($)')\n",
    "ax2.get_figure().suptitle('')\n",
    "\n",
    "# Price vs minimum nights\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "sample = df_clean.sample(min(5000, len(df_clean)))\n",
    "scatter = ax3.scatter(sample['minimum_nights'], sample['price'], alpha=0.4, s=20, c=sample['availability_365'], cmap='viridis')\n",
    "ax3.set_title('Price vs Minimum Nights', fontweight='bold')\n",
    "ax3.set_xlabel('Minimum Nights')\n",
    "ax3.set_ylabel('Price ($)')\n",
    "ax3.set_xlim(0, 500)\n",
    "ax3.set_ylim(0, 500)\n",
    "plt.colorbar(scatter, ax=ax3, label='Availability')\n",
    "\n",
    "# Price vs reviews per month\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "sample_rpm = df_clean[df_clean['reviews_per_month'].notna()].sample(min(5000, len(df_clean)))\n",
    "ax4.scatter(sample_rpm['reviews_per_month'], sample_rpm['price'], alpha=0.4, s=20, color='orange')\n",
    "ax4.set_title('Price vs Reviews Per Month', fontweight='bold')\n",
    "ax4.set_xlabel('Reviews/Month')\n",
    "ax4.set_ylabel('Price ($)')\n",
    "ax4.set_ylim(0, 500)\n",
    "\n",
    "# Price vs host listings\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "sample_host = df_clean[df_clean['calculated_host_listings_count'] <= 50].sample(min(5000, len(df_clean)))\n",
    "ax5.scatter(sample_host['calculated_host_listings_count'], sample_host['price'], alpha=0.4, s=20, color='green')\n",
    "ax5.set_title('Price vs Host Listings Count', fontweight='bold')\n",
    "ax5.set_xlabel('Host Listings Count')\n",
    "ax5.set_ylabel('Price ($)')\n",
    "ax5.set_ylim(0, 500)\n",
    "\n",
    "# Geographic distribution\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "scatter_geo = ax6.scatter(df_clean['longitude'], df_clean['latitude'], alpha=0.3, s=5, c=df_clean['price'], cmap='plasma')\n",
    "ax6.set_title('Geographic Distribution (Colored by Price)', fontweight='bold')\n",
    "ax6.set_xlabel('Longitude')\n",
    "ax6.set_ylabel('Latitude')\n",
    "plt.colorbar(scatter_geo, ax=ax6, label='Price ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/eda/03_bivariate_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: visualizations/eda/03_bivariate_analysis.png\")\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b079a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Correlation Analysis Visualization...\n",
      "✓ Saved: visualizations/eda/04_correlation_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 7: CORRELATION ANALYSIS ====================\n",
    "print(\"\\nGenerating Correlation Analysis Visualization...\")\n",
    "\n",
    "numeric_df = df_clean.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f', \n",
    "            square=True, linewidths=0.5, cbar_kws={'label': 'Correlation'}, ax=ax)\n",
    "ax.set_title('Correlation Matrix - All Numeric Variables', fontweight='bold', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/eda/04_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: visualizations/eda/04_correlation_heatmap.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40315212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing Feature Engineering...\n",
      "✓ Created 10 engineered features\n",
      "  Original columns: 16\n",
      "  New columns: 27\n",
      "✓ Saved: data/processed/data_engineered.csv\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 8: FEATURE ENGINEERING ====================\n",
    "print(\"\\nPerforming Feature Engineering...\")\n",
    "\n",
    "df_engineered = df_clean.copy()\n",
    "\n",
    "# Feature 1: Has reviews\n",
    "df_engineered['has_reviews'] = (df_engineered['number_of_reviews'] > 0).astype(int)\n",
    "\n",
    "# Feature 2: Days since review\n",
    "df_engineered['last_review_date'] = pd.to_datetime(df_engineered['last_review'], errors='coerce')\n",
    "reference_date = pd.Timestamp('2024-12-31')\n",
    "df_engineered['days_since_review'] = (reference_date - df_engineered['last_review_date']).dt.days\n",
    "df_engineered['days_since_review'].fillna(-1, inplace=True)\n",
    "\n",
    "# Feature 3: Price category\n",
    "price_bins = [0, 50, 100, 200, 500, float('inf')]\n",
    "price_labels = ['Budget', 'Economy', 'Mid-Range', 'Premium', 'Luxury']\n",
    "df_engineered['price_category'] = pd.cut(df_engineered['price'], bins=price_bins, labels=price_labels, include_lowest=True)\n",
    "\n",
    "# Feature 4: Availability level\n",
    "df_engineered['availability_level'] = pd.cut(df_engineered['availability_365'],\n",
    "                                              bins=[0, 30, 100, 300, 365],\n",
    "                                              labels=['Low', 'Medium-Low', 'Medium-High', 'High'],\n",
    "                                              include_lowest=True)\n",
    "\n",
    "# Feature 5: Host experience\n",
    "df_engineered['host_experience'] = pd.cut(df_engineered['calculated_host_listings_count'],\n",
    "                                           bins=[0, 1, 3, 10, float('inf')],\n",
    "                                           labels=['New', 'Growing', 'Established', 'Power'],\n",
    "                                           include_lowest=True)\n",
    "\n",
    "# Feature 6: Popularity score\n",
    "df_engineered['popularity_score'] = df_engineered['number_of_reviews'] / (df_engineered['calculated_host_listings_count'] + 1)\n",
    "\n",
    "# Feature 7: Room type encoded\n",
    "room_type_map = {room: idx for idx, room in enumerate(sorted(df_engineered['room_type'].unique()))}\n",
    "df_engineered['room_type_encoded'] = df_engineered['room_type'].map(room_type_map)\n",
    "\n",
    "# Feature 8: Is entire home\n",
    "df_engineered['is_entire_home'] = (df_engineered['room_type'] == 'Entire home/apt').astype(int)\n",
    "\n",
    "# Feature 9: Host multi-listing\n",
    "df_engineered['host_multi_listing'] = (df_engineered['calculated_host_listings_count'] > 1).astype(int)\n",
    "\n",
    "# Feature 10: Neighbourhood encoded\n",
    "neighbourhood_map = {ng: idx for idx, ng in enumerate(df_engineered['neighbourhood_group'].unique())}\n",
    "df_engineered['neighbourhood_encoded'] = df_engineered['neighbourhood_group'].map(neighbourhood_map)\n",
    "\n",
    "print(f\"✓ Created 10 engineered features\")\n",
    "print(f\"  Original columns: {df_clean.shape[1]}\")\n",
    "print(f\"  New columns: {df_engineered.shape[1]}\")\n",
    "\n",
    "# Save engineered data\n",
    "df_engineered.to_csv('data/processed/data_engineered.csv', index=False)\n",
    "print(f\"✓ Saved: data/processed/data_engineered.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a4604a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing Data for Modeling...\n",
      "✓ Data prepared: 39,107 training, 9,777 test samples\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 9: MODELING PREPARATION ====================\n",
    "print(\"\\nPreparing Data for Modeling...\")\n",
    "\n",
    "modeling_features = ['minimum_nights', 'number_of_reviews', 'availability_365', \n",
    "                     'reviews_per_month', 'calculated_host_listings_count', 'latitude', 'longitude',\n",
    "                     'room_type_encoded', 'neighbourhood_encoded', 'has_reviews', \n",
    "                     'days_since_review', 'popularity_score', 'is_entire_home', 'host_multi_listing']\n",
    "\n",
    "df_model = df_engineered[modeling_features + ['price']].dropna()\n",
    "\n",
    "X = df_model[modeling_features]\n",
    "y = df_model['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"✓ Data prepared: {X_train.shape[0]:,} training, {X_test.shape[0]:,} test samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bed8e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Predictive Models...\n",
      "✓ Linear Regression - R²: 0.1251\n",
      "  Saved: models/linear_regression_model.pkl\n",
      "✓ Random Forest - R²: 0.1673\n",
      "✓ Gradient Boosting - R²: 0.1967\n",
      "\n",
      "✓ Best Model: Gradient Boosting\n",
      "  Saved: models/best_model_gradient_boosting.pkl\n",
      "  Saved: models/model_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 10: BUILD MODELS ====================\n",
    "print(\"\\nBuilding Predictive Models...\")\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "y_pred_test_lr = lr_model.predict(X_test_scaled)\n",
    "r2_test_lr = r2_score(y_test, y_pred_test_lr)\n",
    "rmse_test_lr = np.sqrt(mean_squared_error(y_test, y_pred_test_lr))\n",
    "mae_test_lr = mean_absolute_error(y_test, y_pred_test_lr)\n",
    "\n",
    "model_results['Linear Regression'] = {\n",
    "    'Test_R2': r2_test_lr,\n",
    "    'Test_RMSE': rmse_test_lr,\n",
    "    'Test_MAE': mae_test_lr,\n",
    "    'Predictions': y_pred_test_lr\n",
    "}\n",
    "\n",
    "# Save Linear Regression model\n",
    "with open('models/linear_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "\n",
    "print(f\"✓ Linear Regression - R²: {r2_test_lr:.4f}\")\n",
    "print(f\"  Saved: models/linear_regression_model.pkl\")\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_test_rf = rf_model.predict(X_test)\n",
    "r2_test_rf = r2_score(y_test, y_pred_test_rf)\n",
    "rmse_test_rf = np.sqrt(mean_squared_error(y_test, y_pred_test_rf))\n",
    "mae_test_rf = mean_absolute_error(y_test, y_pred_test_rf)\n",
    "\n",
    "model_results['Random Forest'] = {\n",
    "    'Test_R2': r2_test_rf,\n",
    "    'Test_RMSE': rmse_test_rf,\n",
    "    'Test_MAE': mae_test_rf,\n",
    "    'Predictions': y_pred_test_rf,\n",
    "    'Model': rf_model\n",
    "}\n",
    "\n",
    "print(f\"✓ Random Forest - R²: {r2_test_rf:.4f}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_test_gb = gb_model.predict(X_test)\n",
    "r2_test_gb = r2_score(y_test, y_pred_test_gb)\n",
    "rmse_test_gb = np.sqrt(mean_squared_error(y_test, y_pred_test_gb))\n",
    "mae_test_gb = mean_absolute_error(y_test, y_pred_test_gb)\n",
    "\n",
    "model_results['Gradient Boosting'] = {\n",
    "    'Test_R2': r2_test_gb,\n",
    "    'Test_RMSE': rmse_test_gb,\n",
    "    'Test_MAE': mae_test_gb,\n",
    "    'Predictions': y_pred_test_gb,\n",
    "    'Model': gb_model\n",
    "}\n",
    "\n",
    "print(f\"✓ Gradient Boosting - R²: {r2_test_gb:.4f}\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(model_results, key=lambda x: model_results[x]['Test_R2'])\n",
    "print(f\"\\n✓ Best Model: {best_model_name}\")\n",
    "\n",
    "# Save best model\n",
    "best_model_obj = model_results[best_model_name]['Model']\n",
    "with open(f'models/best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model_obj, f)\n",
    "print(f\"  Saved: models/best_model_{best_model_name.lower().replace(' ', '_')}.pkl\")\n",
    "\n",
    "# Save all models metadata\n",
    "model_metadata = {\n",
    "    'best_model': best_model_name,\n",
    "    'all_models_performance': comparison_df.to_dict() if 'comparison_df' in locals() else model_results,\n",
    "    'feature_names': list(X.columns),\n",
    "    'scaler': scaler,\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "with open('models/model_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(model_metadata, f)\n",
    "print(f\"  Saved: models/model_metadata.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "343fc427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Model Comparison Visualizations...\n",
      "✓ Saved: visualizations/modeling/01_model_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 11: MODEL COMPARISON VISUALIZATION ====================\n",
    "print(\"\\nGenerating Model Comparison Visualizations...\")\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': model_results.keys(),\n",
    "    'Test_R²': [model_results[m]['Test_R2'] for m in model_results.keys()],\n",
    "    'Test_RMSE': [model_results[m]['Test_RMSE'] for m in model_results.keys()],\n",
    "    'Test_MAE': [model_results[m]['Test_MAE'] for m in model_results.keys()]\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# R² comparison\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Test_R²'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black')\n",
    "axes[0].set_title('Model Comparison - R² Score', fontweight='bold')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_ylim(0, 1)\n",
    "for i, v in enumerate(comparison_df['Test_R²']):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# RMSE comparison\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['Test_RMSE'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black')\n",
    "axes[1].set_title('Model Comparison - RMSE', fontweight='bold')\n",
    "axes[1].set_ylabel('RMSE ($)')\n",
    "for i, v in enumerate(comparison_df['Test_RMSE']):\n",
    "    axes[1].text(i, v + 5, f'${v:.0f}', ha='center', fontweight='bold')\n",
    "\n",
    "# MAE comparison\n",
    "axes[2].bar(comparison_df['Model'], comparison_df['Test_MAE'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black')\n",
    "axes[2].set_title('Model Comparison - MAE', fontweight='bold')\n",
    "axes[2].set_ylabel('MAE ($)')\n",
    "for i, v in enumerate(comparison_df['Test_MAE']):\n",
    "    axes[2].text(i, v + 2, f'${v:.0f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/modeling/01_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: visualizations/modeling/01_model_comparison.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "494bd6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Best Model Evaluation Visualizations...\n",
      "✓ Saved: visualizations/modeling/02_predictions_vs_actual.png\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 12: BEST MODEL EVALUATION ====================\n",
    "print(\"\\nGenerating Best Model Evaluation Visualizations...\")\n",
    "\n",
    "if best_model_name == 'Gradient Boosting':\n",
    "    y_pred_best = y_pred_test_gb\n",
    "    feature_imp = gb_model.feature_importances_\n",
    "elif best_model_name == 'Random Forest':\n",
    "    y_pred_best = y_pred_test_rf\n",
    "    feature_imp = rf_model.feature_importances_\n",
    "else:\n",
    "    y_pred_best = y_pred_test_lr\n",
    "\n",
    "# Prediction vs Actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred_best, alpha=0.5, s=20, color='steelblue')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_title(f'{best_model_name} - Predicted vs Actual\\nR² = {model_results[best_model_name][\"Test_R2\"]:.4f}', fontweight='bold')\n",
    "axes[0].set_xlabel('Actual Price ($)')\n",
    "axes[0].set_ylabel('Predicted Price ($)')\n",
    "axes[0].set_xlim(0, 500)\n",
    "axes[0].set_ylim(0, 500)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred_best\n",
    "axes[1].scatter(y_pred_best, residuals, alpha=0.5, s=20, color='orange')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_title('Residuals Analysis', fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Price ($)')\n",
    "axes[1].set_ylabel('Residuals ($)')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/modeling/02_predictions_vs_actual.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: visualizations/modeling/02_predictions_vs_actual.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "920be46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Feature Importance Visualization...\n",
      "✓ Saved: visualizations/modeling/03_feature_importance.png\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 13: FEATURE IMPORTANCE ====================\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    print(\"\\nGenerating Feature Importance Visualization...\")\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': feature_imp\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    ax.barh(top_features['Feature'], top_features['Importance'], color='steelblue', edgecolor='black')\n",
    "    ax.set_xlabel('Importance Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'Top 15 Feature Importance - {best_model_name}', fontsize=13, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/modeling/03_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved: visualizations/modeling/03_feature_importance.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c0ad46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing Clustering Analysis...\n",
      "✓ Saved: visualizations/clustering/01_elbow_and_silhouette.png\n",
      "✓ Saved: visualizations/clustering/02_cluster_distributions.png\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "GENERATED FILES:\n",
      "\n",
      "Data Files:\n",
      "  ✓ data/raw/MinoAI dataset.csv (original)\n",
      "  ✓ data/processed/data_cleaned.csv\n",
      "  ✓ data/processed/data_engineered.csv\n",
      "\n",
      "Visualizations (EDA):\n",
      "  ✓ visualizations/eda/01_missing_values.png\n",
      "  ✓ visualizations/eda/02_univariate_analysis.png\n",
      "  ✓ visualizations/eda/03_bivariate_analysis.png\n",
      "  ✓ visualizations/eda/04_correlation_heatmap.png\n",
      "\n",
      "Visualizations (Modeling):\n",
      "  ✓ visualizations/modeling/01_model_comparison.png\n",
      "  ✓ visualizations/modeling/02_predictions_vs_actual.png\n",
      "  ✓ visualizations/modeling/03_feature_importance.png\n",
      "\n",
      "Visualizations (Clustering):\n",
      "  ✓ visualizations/clustering/01_elbow_and_silhouette.png\n",
      "  ✓ visualizations/clustering/02_cluster_distributions.png\n",
      "\n",
      "Trained Models Saved:\n",
      "  ✓ models/linear_regression_model.pkl\n",
      "  ✓ models/random_forest_model.pkl\n",
      "  ✓ models/gradient_boosting_model.pkl\n",
      "  ✓ models/best_model_gradient_boosting.pkl\n",
      "  ✓ models/model_metadata.pkl\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "  Best Model: Gradient Boosting\n",
      "  Test R² Score: 0.1967\n",
      "  Test RMSE: $179.34\n",
      "  Test MAE: $64.76\n",
      "  Optimal Clusters: 3\n",
      "\n",
      "✓ ALL OBJECTIVES COMPLETED SUCCESSFULLY!\n"
     ]
    }
   ],
   "source": [
    "# ==================== CELL 14: CLUSTERING ANALYSIS ====================\n",
    "print(\"\\nPerforming Clustering Analysis...\")\n",
    "\n",
    "clustering_features = ['price', 'minimum_nights', 'number_of_reviews', 'availability_365', 'reviews_per_month']\n",
    "cluster_data = df_engineered[clustering_features].dropna()\n",
    "\n",
    "scaler_cluster = StandardScaler()\n",
    "cluster_scaled = scaler_cluster.fit_transform(cluster_data)\n",
    "\n",
    "# Elbow method\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(cluster_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(cluster_scaled, labels))\n",
    "\n",
    "# Plot clustering metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_title('Elbow Method', fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_title('Silhouette Score (Higher Better)', fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/clustering/01_elbow_and_silhouette.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: visualizations/clustering/01_elbow_and_silhouette.png\")\n",
    "plt.close()\n",
    "\n",
    "# Apply K-means with optimal K\n",
    "optimal_k = 3\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(cluster_scaled)\n",
    "\n",
    "cluster_data['Cluster'] = cluster_labels\n",
    "\n",
    "# Cluster characteristics visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(clustering_features):\n",
    "    ax = axes[idx]\n",
    "    for cluster_id in range(optimal_k):\n",
    "        cluster_mask = cluster_data['Cluster'] == cluster_id\n",
    "        data = cluster_data[cluster_mask][feature]\n",
    "        ax.hist(data, alpha=0.6, bins=30, label=f'Cluster {cluster_id}')\n",
    "    ax.set_title(f'{feature} by Cluster', fontweight='bold')\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "\n",
    "# Remove extra subplot\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/clustering/02_cluster_distributions.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: visualizations/clustering/02_cluster_distributions.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nGENERATED FILES:\")\n",
    "print(\"\\nData Files:\")\n",
    "print(\"  ✓ data/raw/MinoAI dataset.csv (original)\")\n",
    "print(\"  ✓ data/processed/data_cleaned.csv\")\n",
    "print(\"  ✓ data/processed/data_engineered.csv\")\n",
    "\n",
    "print(\"\\nVisualizations (EDA):\")\n",
    "print(\"  ✓ visualizations/eda/01_missing_values.png\")\n",
    "print(\"  ✓ visualizations/eda/02_univariate_analysis.png\")\n",
    "print(\"  ✓ visualizations/eda/03_bivariate_analysis.png\")\n",
    "print(\"  ✓ visualizations/eda/04_correlation_heatmap.png\")\n",
    "\n",
    "print(\"\\nVisualizations (Modeling):\")\n",
    "print(\"  ✓ visualizations/modeling/01_model_comparison.png\")\n",
    "print(\"  ✓ visualizations/modeling/02_predictions_vs_actual.png\")\n",
    "print(\"  ✓ visualizations/modeling/03_feature_importance.png\")\n",
    "\n",
    "print(\"\\nVisualizations (Clustering):\")\n",
    "print(\"  ✓ visualizations/clustering/01_elbow_and_silhouette.png\")\n",
    "print(\"  ✓ visualizations/clustering/02_cluster_distributions.png\")\n",
    "\n",
    "print(\"\\nTrained Models Saved:\")\n",
    "print(\"  ✓ models/linear_regression_model.pkl\")\n",
    "print(\"  ✓ models/random_forest_model.pkl\")\n",
    "print(\"  ✓ models/gradient_boosting_model.pkl\")\n",
    "print(f\"  ✓ models/best_model_{best_model_name.lower().replace(' ', '_')}.pkl\")\n",
    "print(\"  ✓ models/model_metadata.pkl\")\n",
    "\n",
    "print(\"\\nSUMMARY STATISTICS:\")\n",
    "print(f\"  Best Model: {best_model_name}\")\n",
    "print(f\"  Test R² Score: {model_results[best_model_name]['Test_R2']:.4f}\")\n",
    "print(f\"  Test RMSE: ${model_results[best_model_name]['Test_RMSE']:.2f}\")\n",
    "print(f\"  Test MAE: ${model_results[best_model_name]['Test_MAE']:.2f}\")\n",
    "print(f\"  Optimal Clusters: {optimal_k}\")\n",
    "print(f\"\\n✓ ALL OBJECTIVES COMPLETED SUCCESSFULLY!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
